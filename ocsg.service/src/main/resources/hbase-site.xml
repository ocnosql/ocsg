<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
/**
*
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements.  See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership.  The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License.  You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/
-->
<configuration>
<property>
  <name>hbase.regionserver.wal.codec</name>
  <value>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec</value>
</property>
<property>
  <name>hbase.region.server.rpc.scheduler.factory.class</name>
  <value>org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory</value>
  <description>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates</description>
</property>
<property>
  <name>hbase.rpc.controllerfactory.class</name>
  <value>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory</value>
  <description>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates</description>
</property>
<property>
  <name>hbase.master.loadbalancer.class</name>
  <value>org.apache.phoenix.hbase.index.balancer.IndexLoadBalancer</value>
</property>
<property>
  <name>hbase.coprocessor.master.classes</name>
  <value>org.apache.phoenix.hbase.index.master.IndexMasterObserver</value>
</property>
<property>
  <name>hbase.coprocessor.regionserver.classes</name>
  <value>org.apache.hadoop.hbase.regionserver.LocalIndexMerger</value>
</property>
<property>
  <name>hbase.hstore.compaction.kv.max</name>
  <value>100</value>
</property>
<property>
     <name>zookeeper.znode.parent</name>
     <value>/hbase1.1.0</value>
</property>
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://wangkai8/hbase1.1.0</value>
    <description>The directory shared by region servers and into which HBase persists.  The URL should be 'fully-qualified' to include the filesystem scheme.  For example, to specify the HDFS directory '/hbase' where the HDFS instance's namenode is running at namenode.example.org on port 9000, set this value to: hdfs://namenode.example.org:9000/hbase.  By default HBase writes into /tmp.  Change this configuration else all data will be lost on machine restart.</description>
  </property>
  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
    <description>The mode the cluster will be in. Possible values are false for standalone mode and true for distributed mode.  If false, startup will run all HBase and ZooKeeper daemons together in the one JVM.</description>
  </property>
  <property>
    <name>hbase.master.port</name>
    <value>60000</value>
    <description>The port the HBase Master should bind to.</description>
  </property>
  <property>
    <name>hbase.master.info.port</name>
    <value>60010</value>
    <description>The port for the HBase Master web UI.Set to -1 if you do not want a UI instance run.</description>
  </property>
  <property>
    <name>hbase.regionserver.port</name>
    <value>61020</value>
    <description>The port the HBase RegionServer binds to.</description>
  </property>
  <property>
    <name>hbase.regionserver.info.port</name>
    <value>61030</value>
    <description>The port for the HBase RegionServer web UI Set to -1 if you do not want the RegionServer UI to run.</description>
  </property>
  <property>
    <name>hbase.client.write.buffer</name>
    <value>3145728</value>
    <description>Default size of the HTable clien write buffer in bytes.A bigger buffer takes more memory -- on both the client and server side since server instantiates the passed write buffer to process it -- but a larger buffer size reduces the number of RPCs made.For an estimate of server-side memory-used, evaluate hbase.client.write.buffer * hbase.regionserver.handler.count</description>
  </property>
  <property>
    <name>hbase.client.scanner.caching</name>
    <value>100</value>
    <description>Number of rows that will be fetched when calling next on a scanner if it is not served from (local, client) memory. Higher caching values will enable faster scanners but will eat up more memory and some calls of next may take longer and longer times when the cache is empty.Do not set this value such that the time between invocations is greater than the scanner timeout; i.e. hbase.regionserver.lease.period</description>
  </property>
  <property>
    <name>hbase.client.scanner.timeout.period</name>
    <value>180000</value>
    <description>HRegion server lease period in milliseconds. Default is 60 seconds. Clients must report in within this period else they are considered dead.</description>
  </property>
  <property>
    <name>hbase.regionserver.handler.count</name>
    <value>50</value>
    <description>Count of RPC Listener instances spun up on RegionServers.Same property is used by the Master for count of master handlers.Default is 10.</description>
  </property>
  <property>
    <name>hbase.regionserver.msginterval</name>
    <value>10000</value>
    <description>Interval between messages from the RegionServer to Master in milliseconds.</description>
  </property>
  <property>
    <name>hbase.rpc.timeout</name>
    <value>180000</value>
    <description></description>
  </property>
  <property>
    <name>hbase.regionserver.restart.on.zk.expire</name>
    <value>true</value>
    <description></description>
  </property>
  <property>
    <name>hbase.balancer.period</name>
    <value>300000</value>
    <description>Period at which the region balancer runs in the Master.</description>
  </property>
  <property>
    <name>hbase.hregion.memstore.mslab.enabled</name>
    <value>true</value>
    <description>Enables the MemStore-Local Allocation Buffer,a feature which works to prevent heap fragmentation under heavy write loads. This can reduce the frequency of stop-the-world GC pauses on large heaps.</description>
  </property>
  <property>
    <name>hbase.regionserver.global.memstore.size</name>
    <value>0.4</value>
    <description>Maximum size of all memstores in a region server before new updates are blocked and flushes are forced. Defaults to 40% of heap</description>
  </property>
  <property>
    <name>hbase.hregion.max.filesize</name>
    <value>21474836480</value>
    <description>Maximum HStoreFile size. If any one of a column families' HStoreFiles has grown to exceed this value, the hosting HRegion is split in two.Default: 10G.</description>
  </property>
  <property>
    <name>hbase.hstore.compaction.min</name>
    <value>3</value>
    <description>If more than this number of HStoreFiles in any one HStore (one HStoreFile is written per flush of memstore) then a compaction is run to rewrite all HStoreFiles files as one.  Larger number put off compaction but when it runs, it takes longer to complete.</description>
  </property>
  <property>
    <name>hbase.hstore.blockingStoreFiles</name>
    <value>320</value>
    <description>If more than this number of StoreFiles in any one Store (one StoreFile is written per flush of MemStore) then updates are blocked for this HRegion until a compaction is completed, or until hbase.hstore.blockingWaitTime has been exceeded.</description>
  </property>
  <property>
    <name>hbase.hstore.blockingWaitTime</name>
    <value>60000</value>
    <description>The time an HRegion will block updates for after hitting the StoreFile
limit defined by hbase.hstore.blockingStoreFiles.After this time has elapsed, the HRegion will stop blocking updates even if a compaction has not been completed.  Default: 90 seconds.</description>
  </property>
  <property>
    <name>hbase.hstore.compaction.max</name>
    <value>5</value>
    <description>Max number of HStoreFiles to compact per 'minor' compaction.</description>
  </property>
  <property>
    <name>hbase.hregion.majorcompaction</name>
    <value>0</value>
    <description>The time (in miliseconds) between 'major' compactions of all HStoreFiles in a region.  Default: 1 day.Set to 0 to disable automated major compactions.</description>
  </property>
  <property>
    <name>hfile.block.cache.size</name>
    <value>0.4</value>
    <description>Percentage of maximum heap (-Xmx setting) to allocate to block cache used by HFile/StoreFile. Default of 0.25 means allocate 25%.Set to 0 to disable but it's not recommended.</description>
  </property>
  <property>
    <name>zookeeper.session.timeout</name>
    <value>180000</value>
    <description>ZooKeeper session timeout.HBase passes this to the zk quorum as suggested maximum time for a session (This setting becomes zookeeper's 'maxSessionTimeout').  See http://hadoop.apache.org/zookeeper/docs/current/zookeeperProgrammers.html#ch_zkSessions "The client sends a requested timeout, the server responds with the timeout that it can give the client. " In milliseconds.</description>
  </property>
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>wangkai8</value>
    <description>Comma separated list of servers in the ZooKeeper Quorum.For example, "host1.mydomain.com,host2.mydomain.com,host3.mydomain.com".By default this is set to localhost for local and pseudo-distributed modes of operation. For a fully-distributed setup, this should be set to a full list of ZooKeeper quorum servers. If HBASE_MANAGES_ZK is set in hbase-env.sh this is the list of servers which we will start/stop ZooKeeper on.</description>
  </property>
  <property>
    <name>hbase.zookeeper.property.clientPort</name>
    <value>2181</value>
    <description>Property from ZooKeeper's config zoo.cfg. The port at which the clients will connect.</description>
  </property>
  <property>
    <name>hbase.master.maxclockskew</name>
    <value>60000</value>
    <description>ClockOutOfSyncException and RS down</description>
  </property>
  <property>
    <name>hbase.snapshot.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.client.file-block-storage-locations.timeout</name>
    <value>3000</value>
  </property>
  <property>
    <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>hbase.zookeeper.peerport</name>
    <value>4888</value>
  </property>
  <property>
    <name>hbase.zookeeper.leaderport</name>
    <value>5888</value>
  </property>
<property>
        <name>phoenix.query.rowKeyOrderSaltedTable</name>
        <value>true</value>
    </property>
    <property>
        <name>phoenix.query.maxGlobalMemoryPercentage</name>
        <value>50</value>
    </property>

<!--权限认证-->
<!--￼  <property>
    <name>hbase.security.authentication</name>
    <value>simple</value>
  </property>
  <property>
    <name>hbase.security.authorization</name>
    <value>false</value>
  </property>
  <property>
    <name>hbase.coprocessor.master.classes</name>
    <value>org.apache.hadoop.hbase.security.access.AccessController</value>
  </property>
  <property>
    <name>hbase.coprocessor.region.classes</name>
    <value>org.apache.hadoop.hbase.security.access.AccessController</value>
  </property>
  <property>
    <name>hbase.coprocessor.regionserver.classes</name>
    <value>org.apache.hadoop.hbase.security.access.AccessController</value>
  </property>-->
<property>
    <name>hadoop.proxyuser.wangkai8.hosts</name>
    <value>*</value>
</property>
<property>
    <name>hadoop.proxyuser.wangkai8.groups</name>
    <value>*</value>
</property>
</configuration>
